% !TEX root = ../thesis-example.tex
%
\chapter{Introduction}

\section{Motivation}
Machine learning (ML) has become a pivotal tool in decision-making processes across numerous domains,
including healthcare, finance and hiring.
However, as these models increasingly influence critical decisions,
questions about their fairness and the ethical implications of their use have come to the forefront.
Fairness in ML concerns the equitable treatment of individuals or groups,
particularly in the presence of sensitive attributes such as gender, age, race, or socioeconomic status.
These attributes often correlate with historical inequalities or systemic biases embedded in the data.
ML models, designed to optimize accuracy,
learn patterns from this data and may inadvertently exploit these unfair patterns to make predictions.
While leveraging such patterns can improve predictive performance,
it also risks perpetuating or even amplifying existing inequities.
A notable example of this occurred when Amazon's AI recruiting tool,
which was designed to assist in hiring decisions,
exhibited significant gender bias,
favoring male candidates due to biases in historical hiring data,
leading to its eventual scrapping. \cite{amazon_bias}

These challenges of fairness and bias also extend to the field of predictive business process monitoring (PBPM).
Here, ML models play an integral role in making decisions,
whether by predicting outcomes, allocating resources, or streamlining operations.
Addressing fairness in this context requires careful consideration of bias's dual nature.
While biases can lead to discrimination, not all bias is inherently harmful.
In some cases, certain biases may be necessary for the model to achieve its intended purpose.
For instance, sensitive attributes like gender can carry significant information relevant to specific contexts.
In the domain of healthcare, gender differences in biological and hormonal factors
are crucial for determining effective treatments and drug prescriptions.
However, the same attribute might lead to discriminatory outcomes in unrelated contexts,
such as predicting whether a patient's request for treatment will be approved.

A significant obstacle in addressing this issue is the inherent opacity of ML models.
Many models, especially complex ones such as deep neural networks,
produce predictions without providing insight into how those predictions are made.
This lack of interpretability makes it challenging for stakeholders and domain experts
to identify whether a model's reliance on a sensitive attribute aligns with ethical guidelines. 
This presents a dilemma:
either leaving potentially harmful biases unchecked to avoid significantly compromising
the model's predictive performance,
or removing the influence of the sensitive attributes entirely from the model.
The latter approach, while intended to prevent discrimination, can lead to suboptimal predictions,
especially when the sensitive attributes carry critical information relevant to the task.

\section{Problem Statement}
To address this challenge,
this thesis proposes an approach based on knowledge distillation.
Knowledge distillation involves transferring the encapsulated knowledge from a complex model to a simpler,
more interpretable representation.
By applying this technique,
the inner workings of a predictive ML model can be visualized
and understood by human stakeholders and domain experts,
which in turn enables the identification of inherent biases in the model.
Specifically for the proposed approach,
the distilled representation is modified to remove the unwanted bias
by adjusting the way sensitive attributes influence the representation's decision-making process.

The goal of this approach is twofold:
to make the ML model fairer by eliminating the biases that result in unfair treatment
and to maintain its predictive accuracy by preserving helpful ones.
By balancing these objectives,
the proposed methodology seeks to create models that are not only effective
but also aligned with given ethical standards and societal expectations.

\section{Thesis Outline}
Building on the problem statement,
the second chapter provides insight into existing research
concerning the field of bias reduction and fairness in PBPM,
while highlighting the gaps we aim to address.
The third chapter provides background information essential for understanding the thesis.
The fourth chapter details our employed methodology.
The fifth chapter presents the evaluation of the proposed approach,
including empirical results from multiple case studies.
The final chapter concludes the thesis by summarizing the findings
and discussing their implications for fairness in ML.
It acknowledges the limitations of the research and offers suggestions for future work.