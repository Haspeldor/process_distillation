% !TEX root = ../thesis.tex
\chapter{Related Work}
In recent years, the field of fairness in machine learning has gained significant attention,
aiming to address biases and ensure equitable outcomes in predictive systems.
Numerous approaches to quantifying and achieving fairness have been proposed,
targeting various steps in the ML pipeline.
These methods are too plentiful to go over in this thesis,
but an overview can be found in \cite{fairness_ml}.

Comparatively, the existing literature related to fairness specifically in PBPM is quite limited,
although many foundational concepts and challenges concerning fairness can be transferred
from general ML, as proposed in \cite{fairness_overview}.
Accordingly, we will focus on reviewing approaches
that have explicitly been adapted into the field of PBPM.

\cite{fairness_foundation} represents one of the initial efforts to incorporate fairness into PBPM.
This approach utilizes discrimination-aware decision trees,
which extend traditional decision trees by introducing fairness constraints.
Specifically, when the model's predictions exceed a certain threshold of unfairness,
i.e. when they are disproportionately influenced by sensitive attributes like race or gender,
a relabeling technique is applied.
This relabeling adjusts the predictions of the model in a way,
such that the discriminatory impact of sensitive attributes is reduced,
while keeping the loss in predictive accuracy as small as possible.
Although this technique is similar to our methodology as described in section \ref{sec:modification},
there is a key difference:
The relabeling in this method is performed automatically,
based on the assumption that all bias is undesirable and should be eliminated regardless of the context.
In contrast, our approach emphasizes the involvement of domain expert to decide which biases are unwanted,
allowing for the preservation of nuanced, context-dependent correlations.

In \cite{fairness_adversarial},
biases in predictive models are being adressed by leveraging adversarial learning \cite{gan},
a method where two models are trained simultaneously: a predictor and an adversary.
The predictor model focuses on accurate outcomes,
while an adversary model tries to identify sensitive attributes based on the output of the predictor.
The predictor is penalized when the adversary succeeds,
encouraging fairness by reducing reliance on these attributes.
While effective at reducing bias,
this penalty-based approach lacks context sensitivity,
treating all biases as inherently undesirable.
This indiscriminate removal of biases may unintentionally eliminate meaningful correlations,
thereby diminishing the predictive model's utility in specific scenarios
where such relationships are crucial.

Lastly, \cite{fairness_independence} focuses on ensuring independence
between predictions and sensitive group memberships using a composite loss function for training the ML model.
Compared to traditional loss functions, which seek to only optimize the predictive performance of a ML model,
this composite loss function incorporates integral probability metrics \cite{ipm},
in order to find a balance between accuracy and fairness.
However, similar to the other approaches presented before,
the uniform elimination of biases may inadvertently remove desirable correlations,
potentially affecting the utility of the predictive model in certain contexts.

\begin{table}[h!]
    \centering
    \scriptsize
    \renewcommand{\arraystretch}{1.8} % Set default row spacing for data rows
    \setlength{\tabcolsep}{6pt}
    \begin{tabular}{>{\centering\arraybackslash}c | c | c | c }
        \toprule
        \textbf{Work} & \textbf{Predictive Model} & \textbf{Fairness Metrics} & \textbf{Datasets} \\
        \midrule
        \cite{fairness_foundation} & Decision Trees \cite{trees} & \makecell{Demographic Parity \cite{demographic_parity}} & \makecell{Hospital Billing Log \cite{hospital_billing}} \\
        \midrule
        \cite{fairness_adversarial} & \makecell{Fully-connected \\ Neural Networks \cite{backpropagation}, \\ LSTM \cite{lstm}} & \makecell{Shapley Values \cite{shapley}, \\ Equalized Odds \cite{equalized_odds}} & \makecell{BPI Challenge 2013 \cite{bpi_2012}, \\ Simulated Logs \cite{simulated_logs}} \\
        \midrule
        \cite{fairness_independence} & LSTM \cite{lstm} & \makecell{Demographic Parity \cite{demographic_parity}, \\ ABPC and ABCC \cite{fairness_distribution}} & \makecell{Simulated Logs \cite{simulated_logs}} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of related work, highlighting different utilized predictive models, fairness metrics, and datasets.}
    \label{tab:related_work_comparison}
\end{table}

In conclusion, current approaches to fairness in PBPM have made significant strides
but remain somewhat limited by their automated and uniform treatment of bias.
A shift toward an approach that puts more control into the hands of domain experts 
presents a promising direction for creating more context-sensitive models,
that achieve better performance while still making context-based fair decisions.