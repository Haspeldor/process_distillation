% !TEX root = ../thesis.tex
\chapter{Methodology}
\label{sec:methodology}
This chapter will outline the complete pipeline of our methodology.
It describes the process of data gathering and preprocessing,
with a focus on handling bias
causing attributes.
The chapter then explains how the initial ML model is trained,
followed by the application of knowledge distillation to create an interpretable
representation of the model, which can then be modified to address unwanted biases.
The methodology further includes how the predictions of the modified representation
can be used to finetune the original model in order to improve fairness, while maintaining accuracy.
The implementation of our methodology is entirely written in python and can be found in \cite{implementation}.

\section{Data Gathering}
Public real life event logs
containing sensitive attributes are hard to come by.
Additionally, in order to properly highlight the full capacity of our method,
we require the data to have sensitive attributes
that cause both positive and negative bias respectively.
Although \cite{simulated_logs} recently published simulated event logs,
seeking to address the scarcity of fairness-aware datasets in PBPM,
those event logs didn't show the structure we were looking for in this thesis.
Therefore, we will create the necessary data ourselves,
either by simulating a process or by enriching a real life event log with sensitive attributes.

\subsection{Event Log Simulation}
Our simulated event logs are generated by following rule-based transitions within a constructed process model.
Each case begins at a start event and progresses by iteratively selecting the next activity
based on specified transition probabilities, continuing until the end event is reached.
Additionally, each case is assigned randomly generated case attributes, which influence certain transition probabilities.
The sequences of events will then be saved in the form of an event log, as defined in section \ref{sec:event_log}.
An example for a process model, suitable for simulation, is shown in figure \ref{fig:simulated_model}.
Transition probabilities along the sequence flow are either defined generally
or determined by the value of a specific case attribute.
If no sequence flow is defined, the transition probability between activities is set to zero.

% TODO: img model for example

\subsection{Event Log Enrichment}
A common critique of using simulated event logs
is that their complexity may not fully reflect that of event logs derived from real-life processes.
To address this, we augment the publicly available event logs, which lack sensitive attributes,
by adding these attributes ourselves.
This enrichment process is rule-based:
each case is evaluated against specific structural criteria,
and attributes are assigned according to distributions that align with the defined rules.
Detailed examples of these enrichment rules
and their corresponding attribute distributions are provided in section \ref{sec:evaluation}.

\section{Preprocessing}
Since NNs cannot learn from the event log format directly,
we preprocess the data to generate samples in the form of $(x,y)$,
where $x = (x_1, ..., x_n)$ represents the $n$ input features and $y = (y_1, ..., y_k)$ represents the target outcome
out of $k$ activities, as described in section \ref{sec:backpropagation}. 
In order to do this, we extract all possible prefix-outcome pairs from each case in the event log.
The prefix consists of the sequence of events up to a certain point in the case,
while the outcome corresponds to the next activity after the prefix.
Our target $y$ can be directly derived from the next activities by one-hot encoding.

The input $x$ should contain information about both the prefix and the corresponding case attributes.
Since neural networks require inputs of a fixed size,
we handle the varying lengths of prefixes by applying a sliding window of a static size $w$.
If a prefix is shorter than the window size,
we pad the sequence with a special \textit{<PAD>} activity to fill the remaining positions.
Conversely, if a prefix is longer than $w$, we truncate it to include only the most recent $w$ events. 

Case attributes are encoded based on their type.
Categorical attributes are one-hot encoded in the same way as activities,
ensuring that each category is represented as a unique binary vector.
Numerical attributes are normalized using \textbf{min-max scaling},
which rescales their values to fall within the range of 0 to 1,
ensuring that all attributes contribute equally to the model's training process,
preventing numerical attributes with larger magnitudes from dominating those with smaller ones,
as well as categorical attributes.
Given a numerical value $z$, the scaled value $z'$ is calculated as 
\begin{align}
    z' = \frac{z - \textit{min}}{\textit{max} - \textit{min}}
\end{align}
where \textit{min} and \textit{max} refer to the minimum and maximum observed values of the attribute in the dataset.
Finally, we concatenate the sequence of one-hot encoded activities
and the encoded case attributes into the input vector $x$.
An example for the process can be found in figure \ref{fig:preprocessing}

After processing all samples, the last step is to split the samples into a \textbf{training set} and a \textbf{test set}.
For this purpose, we used the common train-test ratio of 80-20.
The training set is used to improve the NN's accuracy through backpropagation,
while the test set is reserved for the final evaluation of the model's performance after training is complete.
It is crucial to keep these sets strictly separated to prevent any information
from the test set leaking into the training set.
Therefore, special care must be taken during the splitting process: \cite{data_split}

\begin{itemize}
\item \textbf{Min-Max Scaling:}  
When performing min-max scaling,
the scaling parameters min and max are derived from just the training set.
This simulates a real-world scenario in which only the values of old training data is known to us,
which ensures that the test set remains independent.
\item \textbf{Instance-Level Leakage:}  
We ensure that all events of a single case are contained either in the training set or the test set, but not both.
Splitting process instances across sets might lead to information leakage,
allowing the model to gain knowledge about test data it would not have access to in a real-world scenario.
\item \textbf{Temporal Leakage:}  
When utilizing timestamp date, one could consider using a chronological split to train on earlier cases and test on later ones.
This setup mimics real-world predictive scenarios where historical data is used to predict outcomes for future events.
In our case, we mostly use simulated event logs, in which there is no chronological difference,
or public event logs, in which the timestamps have been anonymized,
maintaining only the time difference between events. \cite{hospital_billing}
Therefore, a random split can be used instead.
\item \textbf{Cluster Leakage:}  
If the dataset contains groups of highly similar cases, e.g., cases grouped by customer, department, or product line,
assigning entire clusters to either the training or test set prevents correlated information from leaking between the splits.
The datasets we use however are either simulated or anonymized to protect personal information \cite{hospital_billing}.
As a result, no distinct clusters are evident, making a random split both appropriate and sufficient.
\item \textbf{Representative Datasets:}  
Lastly, it's important to ensure that the test set is representative of the dataset's overall diversity.
Specifically, one should avoid creating splits where rare or common cases are disproportionately represented in one set,
as this could skew model evaluation and lead to biased results.
Since we are working with rather large datasets, any bias introduced by random splitting is likely to be minimal.
Moreover, our primary objective is not to achieve maximum accuracy but to enable a fair and consistent comparison between models,
making randomly splitting acceptable for this task.
\end{itemize}

%TODO img table of event log to encoded vector

\section{Training the Neural Network}
\label{sec:training}
After processing the event log into a dataset suitable for machine learning,
we proceed to train a feedforward neural network (NN) using the prepared samples.
The model is implemented using the \textbf{Sequential} class from the Keras library \cite{keras},
which allows for a straightforward layer-by-layer construction of the network.
As mentioned earlier, our primary focus is not on fully optimizing accuracy,
so we did not invest extensive time or effort into selecting and fine-tuning the model's hyperparameters.
The following hyperparameters were chosen, because they delivered consistently good results in practice:

The NN incorporates three hidden layers with 256, 128, and 64 neurons
The choice of progressively smaller layer sizes allows the network to gradually distill
the input features into more abstract representations, facilitating the learning of hierarchical patterns. 
The amount of perceptrons in the input and output layer is dependent on the task at hand,
determined by the dimensions of the processed input and output samples respectively.
The hidden layer uses the ReLu activation function,
while the ouput layer uses softmax activation, as described in section \ref{sec:feedforward}.

For the training process, we used categorical cross-entropy loss in conjunction with the Adam optimizer,
utilizing its default parameters as provided by Keras, with a learning rate of $\alpha = 0.001$.
The model was trained for a total of 10 epochs, with a batch size of 32.
In our experiments, 10 epochs were sufficient for the model to converge without overfitting.
The batch size of 32 was chosen because it offers a practical compromise
between computational efficiency and stable gradient updates.
This configuration enabled the model to learn effectively
and produce satisfactory results within a reasonable training time.

% TODO: img of accuracy over epochs for example

\section{Knowledge Distillation}
After training, the NN is expected to achieve high accuracy but may produce biased predictions,
hidden by its black-box nature.
To uncover these biases and better understand the NN's inner workings,
we train a transparent model that mimics the NN's behavior.

This approach leverages \textbf{knowledge distillation},
a technique originally developed to transfer knowledge from a large,
complex model \textbf{(teacher)} to a smaller,
more efficient model \textbf{(student)} \cite{knowledge_distillation}.
By training the student to replicate the teacher's outputs
knowledge distillation enables faster inference and reduced computational demands
while preserving predictive performance.

In addition to efficiency, knowledge distillation has been adapted for interpretability.
Rather than prioritizing computational simplicity,
a transparent student model, such as a decision tree,
can be trained to approximate the teacher model's predictions.
This allows the decision-making process of the opaque NN
to be examined through the interpretable structure of the student,
providing insights into its behavior while retaining much of its predictive accuracy.

Although in recent studies more sophisticated white-box models
such as \cite{}, or \cite{} have been employed for this purpose,
we opted to use regular decision trees (DTs) as defined in section \ref{sec:dt},
since they are both easy to implement and can be interpreted well
even by domain experts not too familiar in the field of machine learning.

DTs cannot directly utilize the softened output $\hat{y}$ produced by the softmax activation
in the output layer of the neural network,
as they require discrete class labels rather than a probability distribution.
To address this, we apply the \textit{argmax} function to select the class
with the highest probability from the distribution.
Formally, given a training sample $x$,
the distilled target labels $\bar{y}$ are derived from the NN's output as follows:
\begin{align}
    \bar{y} = \textit{argmax}(\textit{NN}(x))
\end{align}

% TODO: img relabeling

\section{Training the Decision Tree}
Using the input samples $x$ from the training dataset
and their corresponding distilled target samples $\bar{y}$, we proceed to train our DT.
For this, we employ the \textbf{DecisionTreeClassifier} class from the Sklearn library \cite{sklearn},
which constructs the DT by recursively partitioning the dataset at each node to minimize Gini impurity,
as outlined in section \ref{sec:dt_training}.

The training hyperparameters for the DT are selected to prioritize interpretability.
The depth of the tree is capped at 10, ensuring the structure remains easy to analyze,
while the number of leaf nodes is restricted to a maximum of 50, further managing complexity.
To avoid overfitting to small fluctuations in the data,
we require each leaf node to represent at least 5 samples,
effectively allowing splits only when 10 or more samples are available.
Additionally, minimal cost-complexity pruning is applied with $\alpha=0.001$,
eliminating splits that provide negligible reductions in impurity.

%TODO: img trained tree

\section{Modification of the Decision Tree}
\label{sec:modification}
Now that the inner workings of the NN have been represented as a decision tree,
we can examine its structure for potential biases.
Specifically, we look for inner nodes that use sensitive attributes as the basis for their splits
and assess whether such usage is justified in the context of the node.
If we determine that one or more nodes unfairly rely on sensitive attributes,
we can modify the decision tree structure by deleting those biased nodes,
thereby removing their influence.
To remove a biased node $v$ while maintaining the defined structure of a binary DT,
we have several possible options:

\begin{itemize}
    \item \textbf{Cutting Branches:} 
    Let $u_1, u_2$ be the children of $v$ and $|S_{u_1}|, |S_{u_2}|$
    the number of training samples associated with each child respectively.
    When removing the node $v$ from the tree,
    the simplest approach would be to replace $v$ with the child node $u_i$
    that has more associated training sampels $|S_{u_i}|$, with $i \in \{1,2\}$.
    Basically, we would eliminate the subtree rooted at the child with less samples.
    This method ensures that the majority of samples are still handled similarly,
    while removing the biased split from the DT.
    However, the subtree and its splits may lose their coherence without the biased split,
    potentially leading to a significant drop in predictive performance after the modification.

    \item \textbf{Retraining Subtrees:}
    An alternative approach when removing $v$ is to delete the entire subtree rooted at $v$
    and retrain it using the samples associated with $v$.  
    The retraining process follows the procedure outlined in section \ref{sec:dt_training},
    stopping when the dataset becomes fully pure or when the subtree reaches a greater depth than the rest of the tree.  
    To prevent the retrained subtree from making the same biased split however,
    any features related to the sensitive attributes used by $v$ must be excluded from the entire subtree.  
    Although this results in more coherent splits,
    there is a possibility that the sensitive attribute could have been used by a descendant of $v$ in a beneficial way,
    which we might not want to eliminate.
    Additionally, the retraining process could introduce new biases related to other sensitive attributes that were not previously present.  
    This means the retraining may need to be repeated multiple times until all unwanted bias is removed.  

    \item \textbf{Manual Modification:}  
    Finally, a domain expert could manually modify the decision tree, its node structure,
    and the associated features and thresholds based on their understanding of how the splits should be structured.
    However, even with domain knowledge,
    manually adjusting the decision tree is challenging to do without substantially compromising prediction accuracy
    and requires significant effort.
\end{itemize}

% TODO: image comparative before and after mod

\section{Finetuning of the Model}
By modifying the DT, our goal was to obtain a predictor that 
makes fairer decisions, while maintaining most of the NN's predictive power.
While we could just use the DT directly for the task of next activity prediction,
deep learning methods generally outperform traditional machine learning approaches,
when it comes to solving complex tasks \cite{ml_comparison}.
In order to combine the more precise predictions of the NN with the fairer decisions of the DT,
we attempt to fine-tune our NN in accordance to the predictions of the DT.

\textbf{Fine-tuning} \cite{fine_tuning} is a transfer learning technique
that involves adapting a pre-trained model to a new task or dataset.
Rather than training a neural network from scratch,
fine-tuning starts with a model that has already been trained on a related task,
leveraging its pre-learned features.
This reduces training time, requires fewer data, and often improves performance
by building on the general representations learned during pre-training.

In our case, we fine-tune the pre-trained NN from the previous section \ref{sec:training}
using a training set modified to enhance fairness.
This fine-tuning dataset retains the same input samples $x$ from the original training set but replaces
the original targets $y$ with modified distilled targets $\tilde{y}$,
derived from the DT's predictions.
However, we can't use the immediate output of the DT, since backpropagation requires a distribution
as its target instead of the discrete label provided by the DT.
To address this, we apply one-hot encoding, transforming
the DT's discrete labels into distribution vectors,
similar to the original targets $y$.
The modified distilled target $\tilde{y}$ for a training sample $x$ is computed
from the DT's prediction as follows:
\begin{align}
    \tilde{y} = \textit{one-hot}(\textit{DT}(x))
\end{align}

When selecting input samples from the relabeled training set during fine-tuning,
we explored several strategies:

\begin{itemize}  
    \item \textbf{Selective Sampling}:  
    Only samples where the original target $y$ differed from the modified distilled target $\tilde{y}$ were included.  
    This approach aimed to accelerate the training process by focusing exclusively on the changed targets.  

    \item \textbf{Complete Sampling}:  
    All samples were included, even when $y$ and $\tilde{y}$ were identical.  
    This ensured the model retained its pre-learned features, although it required a longer time  
    for the model to adapt to the intended changes.  

    \item \textbf{Weighted Sampling}:  
    All samples were presented, but higher weights were assigned to those with changed targets.  
    Samples with heigher weights have an increased impact on the loss function and,
    consequently, on the learning process.
    This offered a compromise between the other two approaches.
\end{itemize}  

Since all of these methods yielded identical or comparable results, we opted for selective sampling,  
focusing only on samples with changed targets, as it offered faster training.  

The training process during fine-tuning was similar to the initial training procedure
described in section \ref{sec:training}, with minor adjustments.
We did not conduct extensive tests regarding the fine-tuning hyperparameters,
since the ideal settings are heavily dependent on both the 
dataset and the architecture of the black-box predictor.
However, these selected hyperparameters yielded satisfactory results in our experiments:
We reduced the Adam optimizer's learning rate $\alpha$ from 0.001
to 0.00001 to balance fairness improvements with maintaining the pre-trained weights.
Additionally, the number of epochs was reduced from 10 to 5,
as the NN quickly achieved perfect accuracy on the modified distilled targets.

%TODO: image finetuning accuracy

%TODO: image: process - complete timeline?